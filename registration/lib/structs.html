<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>registration.lib.structs API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>registration.lib.structs</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="registration.lib.structs.GrowingTensor"><code class="flex name class">
<span>class <span class="ident">GrowingTensor</span></span>
<span>(</span><span>element_shape: Sequence[int], initial_length: int, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GrowingTensor:
    def __init__(self, element_shape: Sequence[int], initial_length: int, **kwargs):
        self.element_size = torch.Size(element_shape)
        self.data = torch.zeros([initial_length] + list(element_shape), **kwargs)
        self.count = 0

    def push_back(self, element: torch.Tensor) -&gt; None:
        assert element.size() == self.element_size
        if self.count &gt;= self.data.size()[0]:
            self.data = torch.cat((self.data, torch.zeros_like(self.data)), dim=0)
        self.data[self.count] = element.to(dtype=self.data.dtype, device=self.data.device)
        self.count += 1

    def get(self) -&gt; torch.Tensor:
        return self.data[:self.count]</code></pre>
</details>
<div class="desc"></div>
<h3>Methods</h3>
<dl>
<dt id="registration.lib.structs.GrowingTensor.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self) -&gt; torch.Tensor:
    return self.data[:self.count]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.GrowingTensor.push_back"><code class="name flex">
<span>def <span class="ident">push_back</span></span>(<span>self, element: torch.Tensor) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def push_back(self, element: torch.Tensor) -&gt; None:
    assert element.size() == self.element_size
    if self.count &gt;= self.data.size()[0]:
        self.data = torch.cat((self.data, torch.zeros_like(self.data)), dim=0)
    self.data[self.count] = element.to(dtype=self.data.dtype, device=self.data.device)
    self.count += 1</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="registration.lib.structs.LinearMapping"><code class="flex name class">
<span>class <span class="ident">LinearMapping</span></span>
<span>(</span><span>intercept: float | torch.Tensor, gradient: float | torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearMapping:
    def __init__(self, intercept: float | torch.Tensor, gradient: float | torch.Tensor):
        self.intercept = intercept
        self.gradient = gradient

    def __call__(self, x: float | torch.Tensor) -&gt; float | torch.Tensor:
        return self.intercept + self.gradient * x</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.LinearRange"><code class="flex name class">
<span>class <span class="ident">LinearRange</span></span>
<span>(</span><span>low: float, high: float)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearRange:
    def __init__(self, low: float, high: float):
        self.low = low
        self.high = high

    def generate_grid(self, count: int, *, device=torch.device(&#39;cpu&#39;)) -&gt; torch.Tensor:
        return torch.linspace(self.low, self.high, count, device=device)

    def generate_tex_coord_grid(self, count: int, *, device=torch.device(&#39;cpu&#39;)) -&gt; torch.Tensor:
        half_cell_size = 0.5 * (self.high - self.low) / float(count)
        return torch.linspace(self.low + half_cell_size, self.high - half_cell_size, count, device=device)

    def get_mapping_from(self, other: &#39;LinearRange&#39;) -&gt; LinearMapping:
        frac: float = (self.high - self.low) / (other.high - other.low)
        return LinearMapping(self.low - frac * other.low, frac)

    def get_spacing(self, count: int) -&gt; float:
        return (self.high - self.low) / float(count - 1)

    def get_tex_coord_spacing(self, count: int) -&gt; float:
        return (self.high - self.low) / float(count)

    def get_centre(self) -&gt; float:
        return .5 * (self.low + self.high)

    @classmethod
    def grid_sample_range(cls):
        return LinearRange(-1., 1.)</code></pre>
</details>
<div class="desc"></div>
<h3>Static methods</h3>
<dl>
<dt id="registration.lib.structs.LinearRange.grid_sample_range"><code class="name flex">
<span>def <span class="ident">grid_sample_range</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="registration.lib.structs.LinearRange.generate_grid"><code class="name flex">
<span>def <span class="ident">generate_grid</span></span>(<span>self, count: int, *, device=device(type='cpu')) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_grid(self, count: int, *, device=torch.device(&#39;cpu&#39;)) -&gt; torch.Tensor:
    return torch.linspace(self.low, self.high, count, device=device)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.LinearRange.generate_tex_coord_grid"><code class="name flex">
<span>def <span class="ident">generate_tex_coord_grid</span></span>(<span>self, count: int, *, device=device(type='cpu')) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_tex_coord_grid(self, count: int, *, device=torch.device(&#39;cpu&#39;)) -&gt; torch.Tensor:
    half_cell_size = 0.5 * (self.high - self.low) / float(count)
    return torch.linspace(self.low + half_cell_size, self.high - half_cell_size, count, device=device)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.LinearRange.get_centre"><code class="name flex">
<span>def <span class="ident">get_centre</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_centre(self) -&gt; float:
    return .5 * (self.low + self.high)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.LinearRange.get_mapping_from"><code class="name flex">
<span>def <span class="ident">get_mapping_from</span></span>(<span>self,<br>other: <a title="registration.lib.structs.LinearRange" href="#registration.lib.structs.LinearRange">LinearRange</a>) ‑> <a title="registration.lib.structs.LinearMapping" href="#registration.lib.structs.LinearMapping">LinearMapping</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_mapping_from(self, other: &#39;LinearRange&#39;) -&gt; LinearMapping:
    frac: float = (self.high - self.low) / (other.high - other.low)
    return LinearMapping(self.low - frac * other.low, frac)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.LinearRange.get_spacing"><code class="name flex">
<span>def <span class="ident">get_spacing</span></span>(<span>self, count: int) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_spacing(self, count: int) -&gt; float:
    return (self.high - self.low) / float(count - 1)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.LinearRange.get_tex_coord_spacing"><code class="name flex">
<span>def <span class="ident">get_tex_coord_spacing</span></span>(<span>self, count: int) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tex_coord_spacing(self, count: int) -&gt; float:
    return (self.high - self.low) / float(count)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="registration.lib.structs.SceneGeometry"><code class="flex name class">
<span>class <span class="ident">SceneGeometry</span></span>
<span>(</span><span>source_distance: float, fixed_image_offset: torch.Tensor = tensor([0., 0.]))</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SceneGeometry(NamedTuple):
    source_distance: float  # [mm]; distance in the positive z-direction from the centre of the detector array
    fixed_image_offset: torch.Tensor = torch.zeros(
        2)  # size (2,): (x, y) [mm]; offset of the fixed image relative to the source

    def source_position(self, *, device=torch.device(&#39;cpu&#39;)):
        return torch.tensor([0., 0., self.source_distance], device=device)

    @classmethod
    def projection_matrix(cls, source_position: torch.Tensor, central_ray: torch.Tensor | None = None) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Generate the projection matrix for the given source position

        :param source_position: [(3,) tensor] the position of the X-ray source
        :param central_ray: [(3,) tensor or None] the vector from the X-ray source to the closest point on the detector array. If none, the detector array is assumed to be the x-y plane.
        :return: [(4, 4) tensor] The projection matrix P that projects points in homogeneous coordinates away from the given source position onto the plane of the detector array, as given by the central ray.
        &#34;&#34;&#34;
        device = source_position.device

        if central_ray is None:
            central_ray = torch.tensor([0., 0., - source_position[2]], device=device)

        assert central_ray.device == device
        assert source_position.size() == torch.Size([3])
        assert central_ray.size() == torch.Size([3])

        m_matrix: torch.Tensor = torch.outer(torch.hstack((source_position, torch.tensor([1.], device=device))),
                                             central_ray) + torch.dot(central_ray, central_ray) * torch.vstack(
            (torch.eye(3, device=device), torch.zeros((1, 3), device=device)))

        return torch.hstack((m_matrix, -torch.matmul(m_matrix, source_position.t().unsqueeze(-1))))</code></pre>
</details>
<div class="desc"><p>SceneGeometry(source_distance, fixed_image_offset)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="registration.lib.structs.SceneGeometry.projection_matrix"><code class="name flex">
<span>def <span class="ident">projection_matrix</span></span>(<span>source_position: torch.Tensor, central_ray: torch.Tensor | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Generate the projection matrix for the given source position</p>
<p>:param source_position: [(3,) tensor] the position of the X-ray source
:param central_ray: [(3,) tensor or None] the vector from the X-ray source to the closest point on the detector array. If none, the detector array is assumed to be the x-y plane.
:return: [(4, 4) tensor] The projection matrix P that projects points in homogeneous coordinates away from the given source position onto the plane of the detector array, as given by the central ray.</p></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="registration.lib.structs.SceneGeometry.fixed_image_offset"><code class="name">var <span class="ident">fixed_image_offset</span> : torch.Tensor</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SceneGeometry(NamedTuple):
    source_distance: float  # [mm]; distance in the positive z-direction from the centre of the detector array
    fixed_image_offset: torch.Tensor = torch.zeros(
        2)  # size (2,): (x, y) [mm]; offset of the fixed image relative to the source

    def source_position(self, *, device=torch.device(&#39;cpu&#39;)):
        return torch.tensor([0., 0., self.source_distance], device=device)

    @classmethod
    def projection_matrix(cls, source_position: torch.Tensor, central_ray: torch.Tensor | None = None) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Generate the projection matrix for the given source position

        :param source_position: [(3,) tensor] the position of the X-ray source
        :param central_ray: [(3,) tensor or None] the vector from the X-ray source to the closest point on the detector array. If none, the detector array is assumed to be the x-y plane.
        :return: [(4, 4) tensor] The projection matrix P that projects points in homogeneous coordinates away from the given source position onto the plane of the detector array, as given by the central ray.
        &#34;&#34;&#34;
        device = source_position.device

        if central_ray is None:
            central_ray = torch.tensor([0., 0., - source_position[2]], device=device)

        assert central_ray.device == device
        assert source_position.size() == torch.Size([3])
        assert central_ray.size() == torch.Size([3])

        m_matrix: torch.Tensor = torch.outer(torch.hstack((source_position, torch.tensor([1.], device=device))),
                                             central_ray) + torch.dot(central_ray, central_ray) * torch.vstack(
            (torch.eye(3, device=device), torch.zeros((1, 3), device=device)))

        return torch.hstack((m_matrix, -torch.matmul(m_matrix, source_position.t().unsqueeze(-1))))</code></pre>
</details>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
<dt id="registration.lib.structs.SceneGeometry.source_distance"><code class="name">var <span class="ident">source_distance</span> : float</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SceneGeometry(NamedTuple):
    source_distance: float  # [mm]; distance in the positive z-direction from the centre of the detector array
    fixed_image_offset: torch.Tensor = torch.zeros(
        2)  # size (2,): (x, y) [mm]; offset of the fixed image relative to the source

    def source_position(self, *, device=torch.device(&#39;cpu&#39;)):
        return torch.tensor([0., 0., self.source_distance], device=device)

    @classmethod
    def projection_matrix(cls, source_position: torch.Tensor, central_ray: torch.Tensor | None = None) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Generate the projection matrix for the given source position

        :param source_position: [(3,) tensor] the position of the X-ray source
        :param central_ray: [(3,) tensor or None] the vector from the X-ray source to the closest point on the detector array. If none, the detector array is assumed to be the x-y plane.
        :return: [(4, 4) tensor] The projection matrix P that projects points in homogeneous coordinates away from the given source position onto the plane of the detector array, as given by the central ray.
        &#34;&#34;&#34;
        device = source_position.device

        if central_ray is None:
            central_ray = torch.tensor([0., 0., - source_position[2]], device=device)

        assert central_ray.device == device
        assert source_position.size() == torch.Size([3])
        assert central_ray.size() == torch.Size([3])

        m_matrix: torch.Tensor = torch.outer(torch.hstack((source_position, torch.tensor([1.], device=device))),
                                             central_ray) + torch.dot(central_ray, central_ray) * torch.vstack(
            (torch.eye(3, device=device), torch.zeros((1, 3), device=device)))

        return torch.hstack((m_matrix, -torch.matmul(m_matrix, source_position.t().unsqueeze(-1))))</code></pre>
</details>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="registration.lib.structs.SceneGeometry.source_position"><code class="name flex">
<span>def <span class="ident">source_position</span></span>(<span>self, *, device=device(type='cpu'))</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def source_position(self, *, device=torch.device(&#39;cpu&#39;)):
    return torch.tensor([0., 0., self.source_distance], device=device)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="registration.lib.structs.Sinogram2dGrid"><code class="flex name class">
<span>class <span class="ident">Sinogram2dGrid</span></span>
<span>(</span><span>phi: torch.Tensor, r: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sinogram2dGrid(NamedTuple):
    phi: torch.Tensor
    r: torch.Tensor

    def to(self, **kwargs) -&gt; &#39;Sinogram2dGrid&#39;:
        return Sinogram2dGrid(self.phi.to(**kwargs), self.r.to(**kwargs))

    def device_consistent(self) -&gt; bool:
        return self.phi.device == self.r.device

    def size_consistent(self) -&gt; bool:
        return self.phi.size() == self.r.size()

    def shifted(self, offset: torch.Tensor) -&gt; &#39;Sinogram2dGrid&#39;:
        assert offset.size() == torch.Size([2])
        cp = self.phi.cos()
        sp = self.phi.sin()
        unit = torch.stack((cp, sp), dim=-1)
        del cp, sp
        delta = torch.einsum(&#34;...i, i -&gt; ...&#34;, unit, offset.to(device=unit.device, dtype=unit.dtype))
        del unit
        return Sinogram2dGrid(self.phi, self.r - delta)

    @classmethod
    def linear_from_range(cls, sinogram_range: Sinogram2dRange, counts: int | Tuple[int, int] | torch.Size, *,
                          device=torch.device(&#34;cpu&#34;)) -&gt; &#39;Sinogram2dGrid&#39;:
        if isinstance(counts, int):
            counts = (counts, counts)
        phis = torch.linspace(sinogram_range.phi.low, sinogram_range.phi.high, counts[0], device=device)
        rs = torch.linspace(sinogram_range.r.low, sinogram_range.r.high, counts[1], device=device)
        phis, rs = torch.meshgrid(phis, rs)
        return Sinogram2dGrid(phis, rs)</code></pre>
</details>
<div class="desc"><p>Sinogram2dGrid(phi, r)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="registration.lib.structs.Sinogram2dGrid.linear_from_range"><code class="name flex">
<span>def <span class="ident">linear_from_range</span></span>(<span>sinogram_range: <a title="registration.lib.structs.Sinogram2dRange" href="#registration.lib.structs.Sinogram2dRange">Sinogram2dRange</a>,<br>counts: int | Tuple[int, int] | torch.Size,<br>*,<br>device=device(type='cpu')) ‑> <a title="registration.lib.structs.Sinogram2dGrid" href="#registration.lib.structs.Sinogram2dGrid">Sinogram2dGrid</a></span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="registration.lib.structs.Sinogram2dGrid.phi"><code class="name">var <span class="ident">phi</span> : torch.Tensor</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sinogram2dGrid(NamedTuple):
    phi: torch.Tensor
    r: torch.Tensor

    def to(self, **kwargs) -&gt; &#39;Sinogram2dGrid&#39;:
        return Sinogram2dGrid(self.phi.to(**kwargs), self.r.to(**kwargs))

    def device_consistent(self) -&gt; bool:
        return self.phi.device == self.r.device

    def size_consistent(self) -&gt; bool:
        return self.phi.size() == self.r.size()

    def shifted(self, offset: torch.Tensor) -&gt; &#39;Sinogram2dGrid&#39;:
        assert offset.size() == torch.Size([2])
        cp = self.phi.cos()
        sp = self.phi.sin()
        unit = torch.stack((cp, sp), dim=-1)
        del cp, sp
        delta = torch.einsum(&#34;...i, i -&gt; ...&#34;, unit, offset.to(device=unit.device, dtype=unit.dtype))
        del unit
        return Sinogram2dGrid(self.phi, self.r - delta)

    @classmethod
    def linear_from_range(cls, sinogram_range: Sinogram2dRange, counts: int | Tuple[int, int] | torch.Size, *,
                          device=torch.device(&#34;cpu&#34;)) -&gt; &#39;Sinogram2dGrid&#39;:
        if isinstance(counts, int):
            counts = (counts, counts)
        phis = torch.linspace(sinogram_range.phi.low, sinogram_range.phi.high, counts[0], device=device)
        rs = torch.linspace(sinogram_range.r.low, sinogram_range.r.high, counts[1], device=device)
        phis, rs = torch.meshgrid(phis, rs)
        return Sinogram2dGrid(phis, rs)</code></pre>
</details>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
<dt id="registration.lib.structs.Sinogram2dGrid.r"><code class="name">var <span class="ident">r</span> : torch.Tensor</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sinogram2dGrid(NamedTuple):
    phi: torch.Tensor
    r: torch.Tensor

    def to(self, **kwargs) -&gt; &#39;Sinogram2dGrid&#39;:
        return Sinogram2dGrid(self.phi.to(**kwargs), self.r.to(**kwargs))

    def device_consistent(self) -&gt; bool:
        return self.phi.device == self.r.device

    def size_consistent(self) -&gt; bool:
        return self.phi.size() == self.r.size()

    def shifted(self, offset: torch.Tensor) -&gt; &#39;Sinogram2dGrid&#39;:
        assert offset.size() == torch.Size([2])
        cp = self.phi.cos()
        sp = self.phi.sin()
        unit = torch.stack((cp, sp), dim=-1)
        del cp, sp
        delta = torch.einsum(&#34;...i, i -&gt; ...&#34;, unit, offset.to(device=unit.device, dtype=unit.dtype))
        del unit
        return Sinogram2dGrid(self.phi, self.r - delta)

    @classmethod
    def linear_from_range(cls, sinogram_range: Sinogram2dRange, counts: int | Tuple[int, int] | torch.Size, *,
                          device=torch.device(&#34;cpu&#34;)) -&gt; &#39;Sinogram2dGrid&#39;:
        if isinstance(counts, int):
            counts = (counts, counts)
        phis = torch.linspace(sinogram_range.phi.low, sinogram_range.phi.high, counts[0], device=device)
        rs = torch.linspace(sinogram_range.r.low, sinogram_range.r.high, counts[1], device=device)
        phis, rs = torch.meshgrid(phis, rs)
        return Sinogram2dGrid(phis, rs)</code></pre>
</details>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="registration.lib.structs.Sinogram2dGrid.device_consistent"><code class="name flex">
<span>def <span class="ident">device_consistent</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def device_consistent(self) -&gt; bool:
    return self.phi.device == self.r.device</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Sinogram2dGrid.shifted"><code class="name flex">
<span>def <span class="ident">shifted</span></span>(<span>self, offset: torch.Tensor) ‑> <a title="registration.lib.structs.Sinogram2dGrid" href="#registration.lib.structs.Sinogram2dGrid">Sinogram2dGrid</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shifted(self, offset: torch.Tensor) -&gt; &#39;Sinogram2dGrid&#39;:
    assert offset.size() == torch.Size([2])
    cp = self.phi.cos()
    sp = self.phi.sin()
    unit = torch.stack((cp, sp), dim=-1)
    del cp, sp
    delta = torch.einsum(&#34;...i, i -&gt; ...&#34;, unit, offset.to(device=unit.device, dtype=unit.dtype))
    del unit
    return Sinogram2dGrid(self.phi, self.r - delta)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Sinogram2dGrid.size_consistent"><code class="name flex">
<span>def <span class="ident">size_consistent</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def size_consistent(self) -&gt; bool:
    return self.phi.size() == self.r.size()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Sinogram2dGrid.to"><code class="name flex">
<span>def <span class="ident">to</span></span>(<span>self, **kwargs) ‑> <a title="registration.lib.structs.Sinogram2dGrid" href="#registration.lib.structs.Sinogram2dGrid">Sinogram2dGrid</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to(self, **kwargs) -&gt; &#39;Sinogram2dGrid&#39;:
    return Sinogram2dGrid(self.phi.to(**kwargs), self.r.to(**kwargs))</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="registration.lib.structs.Sinogram2dRange"><code class="flex name class">
<span>class <span class="ident">Sinogram2dRange</span></span>
<span>(</span><span>phi: <a title="registration.lib.structs.LinearRange" href="#registration.lib.structs.LinearRange">LinearRange</a>,<br>r: <a title="registration.lib.structs.LinearRange" href="#registration.lib.structs.LinearRange">LinearRange</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sinogram2dRange(NamedTuple):
    phi: LinearRange
    r: LinearRange</code></pre>
</details>
<div class="desc"><p>Sinogram2dRange(phi, r)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="registration.lib.structs.Sinogram2dRange.phi"><code class="name">var <span class="ident">phi</span> : <a title="registration.lib.structs.LinearRange" href="#registration.lib.structs.LinearRange">LinearRange</a></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sinogram2dRange(NamedTuple):
    phi: LinearRange
    r: LinearRange</code></pre>
</details>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
<dt id="registration.lib.structs.Sinogram2dRange.r"><code class="name">var <span class="ident">r</span> : <a title="registration.lib.structs.LinearRange" href="#registration.lib.structs.LinearRange">LinearRange</a></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sinogram2dRange(NamedTuple):
    phi: LinearRange
    r: LinearRange</code></pre>
</details>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
</dl>
</dd>
<dt id="registration.lib.structs.Sinogram3dGrid"><code class="flex name class">
<span>class <span class="ident">Sinogram3dGrid</span></span>
<span>(</span><span>phi: torch.Tensor, theta: torch.Tensor, r: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sinogram3dGrid(NamedTuple):
    phi: torch.Tensor
    theta: torch.Tensor
    r: torch.Tensor

    def to(self, **kwargs) -&gt; &#39;Sinogram3dGrid&#39;:
        return Sinogram3dGrid(self.phi.to(**kwargs), self.theta.to(**kwargs), self.r.to(**kwargs))

    def device_consistent(self) -&gt; bool:
        return self.phi.device == self.theta.device and self.theta.device == self.r.device

    def size_consistent(self) -&gt; bool:
        return self.phi.size() == self.theta.size() and self.theta.size() == self.r.size()

    def unflip(self) -&gt; &#39;Sinogram3dGrid&#39;:
        assert self.size_consistent()
        assert self.device_consistent()

        theta_div = torch.div(self.theta + .5 * torch.pi, torch.pi, rounding_mode=&#34;floor&#34;)
        theta_flip = torch.fmod(theta_div.to(dtype=torch.int32).abs(), 2).to(dtype=torch.bool)
        phi_div = torch.div(self.phi + .5 * torch.pi, torch.pi, rounding_mode=&#34;floor&#34;)
        phi_flip = torch.fmod(phi_div.to(dtype=torch.int32).abs(), 2).to(dtype=torch.bool)

        ret_theta = self.theta - torch.pi * theta_div

        del theta_div

        ret_phi = self.phi - torch.pi * phi_div

        del phi_div

        ret_theta[torch.logical_and(phi_flip, torch.logical_not(theta_flip))] *= -1.
        ret_r = self.r.clone()
        ret_r[torch.logical_xor(theta_flip, phi_flip)] *= -1.

        del theta_flip, phi_flip

        return Sinogram3dGrid(ret_phi, ret_theta, ret_r)

    # @classmethod  # def fibonacci_from_r_range(cls, r_range: LinearRange, r_count: int, *, spiral_count: int | None = None,  #                            device=torch.device(&#34;cpu&#34;)) -&gt; &#39;Sinogram3dGrid&#39;:  #     if spiral_count is None:  #         spiral_count = r_count * r_count  #     rs = torch.linspace(r_range.low, r_range.high, r_count, device=device)  #     spiral_indices = torch.arange(spiral_count, dtype=torch.float32)  #     two_pi_phi_inverse = 4. * torch.pi / (1. + torch.sqrt(torch.tensor([5.])))  #     thetas = (1. - 2. * spiral_indices / float(spiral_count)).asin()  #     phis = torch.fmod(spiral_indices * two_pi_phi_inverse + torch.pi, 2. * torch.pi) - torch.pi  #     rs = rs.repeat(spiral_count, 1)  #     thetas = thetas.unsqueeze(-1).repeat(1, r_count)  #     phis = phis.unsqueeze(-1).repeat(1, r_count)  #     return Sinogram3dGrid(phis, thetas, rs)</code></pre>
</details>
<div class="desc"><p>Sinogram3dGrid(phi, theta, r)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="registration.lib.structs.Sinogram3dGrid.phi"><code class="name">var <span class="ident">phi</span> : torch.Tensor</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sinogram3dGrid(NamedTuple):
    phi: torch.Tensor
    theta: torch.Tensor
    r: torch.Tensor

    def to(self, **kwargs) -&gt; &#39;Sinogram3dGrid&#39;:
        return Sinogram3dGrid(self.phi.to(**kwargs), self.theta.to(**kwargs), self.r.to(**kwargs))

    def device_consistent(self) -&gt; bool:
        return self.phi.device == self.theta.device and self.theta.device == self.r.device

    def size_consistent(self) -&gt; bool:
        return self.phi.size() == self.theta.size() and self.theta.size() == self.r.size()

    def unflip(self) -&gt; &#39;Sinogram3dGrid&#39;:
        assert self.size_consistent()
        assert self.device_consistent()

        theta_div = torch.div(self.theta + .5 * torch.pi, torch.pi, rounding_mode=&#34;floor&#34;)
        theta_flip = torch.fmod(theta_div.to(dtype=torch.int32).abs(), 2).to(dtype=torch.bool)
        phi_div = torch.div(self.phi + .5 * torch.pi, torch.pi, rounding_mode=&#34;floor&#34;)
        phi_flip = torch.fmod(phi_div.to(dtype=torch.int32).abs(), 2).to(dtype=torch.bool)

        ret_theta = self.theta - torch.pi * theta_div

        del theta_div

        ret_phi = self.phi - torch.pi * phi_div

        del phi_div

        ret_theta[torch.logical_and(phi_flip, torch.logical_not(theta_flip))] *= -1.
        ret_r = self.r.clone()
        ret_r[torch.logical_xor(theta_flip, phi_flip)] *= -1.

        del theta_flip, phi_flip

        return Sinogram3dGrid(ret_phi, ret_theta, ret_r)

    # @classmethod  # def fibonacci_from_r_range(cls, r_range: LinearRange, r_count: int, *, spiral_count: int | None = None,  #                            device=torch.device(&#34;cpu&#34;)) -&gt; &#39;Sinogram3dGrid&#39;:  #     if spiral_count is None:  #         spiral_count = r_count * r_count  #     rs = torch.linspace(r_range.low, r_range.high, r_count, device=device)  #     spiral_indices = torch.arange(spiral_count, dtype=torch.float32)  #     two_pi_phi_inverse = 4. * torch.pi / (1. + torch.sqrt(torch.tensor([5.])))  #     thetas = (1. - 2. * spiral_indices / float(spiral_count)).asin()  #     phis = torch.fmod(spiral_indices * two_pi_phi_inverse + torch.pi, 2. * torch.pi) - torch.pi  #     rs = rs.repeat(spiral_count, 1)  #     thetas = thetas.unsqueeze(-1).repeat(1, r_count)  #     phis = phis.unsqueeze(-1).repeat(1, r_count)  #     return Sinogram3dGrid(phis, thetas, rs)</code></pre>
</details>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
<dt id="registration.lib.structs.Sinogram3dGrid.r"><code class="name">var <span class="ident">r</span> : torch.Tensor</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sinogram3dGrid(NamedTuple):
    phi: torch.Tensor
    theta: torch.Tensor
    r: torch.Tensor

    def to(self, **kwargs) -&gt; &#39;Sinogram3dGrid&#39;:
        return Sinogram3dGrid(self.phi.to(**kwargs), self.theta.to(**kwargs), self.r.to(**kwargs))

    def device_consistent(self) -&gt; bool:
        return self.phi.device == self.theta.device and self.theta.device == self.r.device

    def size_consistent(self) -&gt; bool:
        return self.phi.size() == self.theta.size() and self.theta.size() == self.r.size()

    def unflip(self) -&gt; &#39;Sinogram3dGrid&#39;:
        assert self.size_consistent()
        assert self.device_consistent()

        theta_div = torch.div(self.theta + .5 * torch.pi, torch.pi, rounding_mode=&#34;floor&#34;)
        theta_flip = torch.fmod(theta_div.to(dtype=torch.int32).abs(), 2).to(dtype=torch.bool)
        phi_div = torch.div(self.phi + .5 * torch.pi, torch.pi, rounding_mode=&#34;floor&#34;)
        phi_flip = torch.fmod(phi_div.to(dtype=torch.int32).abs(), 2).to(dtype=torch.bool)

        ret_theta = self.theta - torch.pi * theta_div

        del theta_div

        ret_phi = self.phi - torch.pi * phi_div

        del phi_div

        ret_theta[torch.logical_and(phi_flip, torch.logical_not(theta_flip))] *= -1.
        ret_r = self.r.clone()
        ret_r[torch.logical_xor(theta_flip, phi_flip)] *= -1.

        del theta_flip, phi_flip

        return Sinogram3dGrid(ret_phi, ret_theta, ret_r)

    # @classmethod  # def fibonacci_from_r_range(cls, r_range: LinearRange, r_count: int, *, spiral_count: int | None = None,  #                            device=torch.device(&#34;cpu&#34;)) -&gt; &#39;Sinogram3dGrid&#39;:  #     if spiral_count is None:  #         spiral_count = r_count * r_count  #     rs = torch.linspace(r_range.low, r_range.high, r_count, device=device)  #     spiral_indices = torch.arange(spiral_count, dtype=torch.float32)  #     two_pi_phi_inverse = 4. * torch.pi / (1. + torch.sqrt(torch.tensor([5.])))  #     thetas = (1. - 2. * spiral_indices / float(spiral_count)).asin()  #     phis = torch.fmod(spiral_indices * two_pi_phi_inverse + torch.pi, 2. * torch.pi) - torch.pi  #     rs = rs.repeat(spiral_count, 1)  #     thetas = thetas.unsqueeze(-1).repeat(1, r_count)  #     phis = phis.unsqueeze(-1).repeat(1, r_count)  #     return Sinogram3dGrid(phis, thetas, rs)</code></pre>
</details>
<div class="desc"><p>Alias for field number 2</p></div>
</dd>
<dt id="registration.lib.structs.Sinogram3dGrid.theta"><code class="name">var <span class="ident">theta</span> : torch.Tensor</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sinogram3dGrid(NamedTuple):
    phi: torch.Tensor
    theta: torch.Tensor
    r: torch.Tensor

    def to(self, **kwargs) -&gt; &#39;Sinogram3dGrid&#39;:
        return Sinogram3dGrid(self.phi.to(**kwargs), self.theta.to(**kwargs), self.r.to(**kwargs))

    def device_consistent(self) -&gt; bool:
        return self.phi.device == self.theta.device and self.theta.device == self.r.device

    def size_consistent(self) -&gt; bool:
        return self.phi.size() == self.theta.size() and self.theta.size() == self.r.size()

    def unflip(self) -&gt; &#39;Sinogram3dGrid&#39;:
        assert self.size_consistent()
        assert self.device_consistent()

        theta_div = torch.div(self.theta + .5 * torch.pi, torch.pi, rounding_mode=&#34;floor&#34;)
        theta_flip = torch.fmod(theta_div.to(dtype=torch.int32).abs(), 2).to(dtype=torch.bool)
        phi_div = torch.div(self.phi + .5 * torch.pi, torch.pi, rounding_mode=&#34;floor&#34;)
        phi_flip = torch.fmod(phi_div.to(dtype=torch.int32).abs(), 2).to(dtype=torch.bool)

        ret_theta = self.theta - torch.pi * theta_div

        del theta_div

        ret_phi = self.phi - torch.pi * phi_div

        del phi_div

        ret_theta[torch.logical_and(phi_flip, torch.logical_not(theta_flip))] *= -1.
        ret_r = self.r.clone()
        ret_r[torch.logical_xor(theta_flip, phi_flip)] *= -1.

        del theta_flip, phi_flip

        return Sinogram3dGrid(ret_phi, ret_theta, ret_r)

    # @classmethod  # def fibonacci_from_r_range(cls, r_range: LinearRange, r_count: int, *, spiral_count: int | None = None,  #                            device=torch.device(&#34;cpu&#34;)) -&gt; &#39;Sinogram3dGrid&#39;:  #     if spiral_count is None:  #         spiral_count = r_count * r_count  #     rs = torch.linspace(r_range.low, r_range.high, r_count, device=device)  #     spiral_indices = torch.arange(spiral_count, dtype=torch.float32)  #     two_pi_phi_inverse = 4. * torch.pi / (1. + torch.sqrt(torch.tensor([5.])))  #     thetas = (1. - 2. * spiral_indices / float(spiral_count)).asin()  #     phis = torch.fmod(spiral_indices * two_pi_phi_inverse + torch.pi, 2. * torch.pi) - torch.pi  #     rs = rs.repeat(spiral_count, 1)  #     thetas = thetas.unsqueeze(-1).repeat(1, r_count)  #     phis = phis.unsqueeze(-1).repeat(1, r_count)  #     return Sinogram3dGrid(phis, thetas, rs)</code></pre>
</details>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="registration.lib.structs.Sinogram3dGrid.device_consistent"><code class="name flex">
<span>def <span class="ident">device_consistent</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def device_consistent(self) -&gt; bool:
    return self.phi.device == self.theta.device and self.theta.device == self.r.device</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Sinogram3dGrid.size_consistent"><code class="name flex">
<span>def <span class="ident">size_consistent</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def size_consistent(self) -&gt; bool:
    return self.phi.size() == self.theta.size() and self.theta.size() == self.r.size()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Sinogram3dGrid.to"><code class="name flex">
<span>def <span class="ident">to</span></span>(<span>self, **kwargs) ‑> <a title="registration.lib.structs.Sinogram3dGrid" href="#registration.lib.structs.Sinogram3dGrid">Sinogram3dGrid</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to(self, **kwargs) -&gt; &#39;Sinogram3dGrid&#39;:
    return Sinogram3dGrid(self.phi.to(**kwargs), self.theta.to(**kwargs), self.r.to(**kwargs))</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Sinogram3dGrid.unflip"><code class="name flex">
<span>def <span class="ident">unflip</span></span>(<span>self) ‑> <a title="registration.lib.structs.Sinogram3dGrid" href="#registration.lib.structs.Sinogram3dGrid">Sinogram3dGrid</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unflip(self) -&gt; &#39;Sinogram3dGrid&#39;:
    assert self.size_consistent()
    assert self.device_consistent()

    theta_div = torch.div(self.theta + .5 * torch.pi, torch.pi, rounding_mode=&#34;floor&#34;)
    theta_flip = torch.fmod(theta_div.to(dtype=torch.int32).abs(), 2).to(dtype=torch.bool)
    phi_div = torch.div(self.phi + .5 * torch.pi, torch.pi, rounding_mode=&#34;floor&#34;)
    phi_flip = torch.fmod(phi_div.to(dtype=torch.int32).abs(), 2).to(dtype=torch.bool)

    ret_theta = self.theta - torch.pi * theta_div

    del theta_div

    ret_phi = self.phi - torch.pi * phi_div

    del phi_div

    ret_theta[torch.logical_and(phi_flip, torch.logical_not(theta_flip))] *= -1.
    ret_r = self.r.clone()
    ret_r[torch.logical_xor(theta_flip, phi_flip)] *= -1.

    del theta_flip, phi_flip

    return Sinogram3dGrid(ret_phi, ret_theta, ret_r)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="registration.lib.structs.Transformation"><code class="flex name class">
<span>class <span class="ident">Transformation</span></span>
<span>(</span><span>rotation: torch.Tensor, translation: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Transformation(NamedTuple):
    rotation: torch.Tensor
    translation: torch.Tensor

    def inverse(self) -&gt; &#39;Transformation&#39;:
        r_inverse = kornia.geometry.conversions.axis_angle_to_rotation_matrix(-self.rotation.unsqueeze(0))[0]
        r_inverse_t = torch.einsum(&#39;kl,...l-&gt;...k&#39;, r_inverse, self.translation.unsqueeze(0))[0]
        return Transformation(-self.rotation, -r_inverse_t)

    def get_h(self, *, device=torch.device(&#39;cpu&#39;)) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        :param device:
        :return: [(4, 4) tensor] The homogenous affine transformation matrix H corresponding to this transformation.
        Stored column-major.
        &#34;&#34;&#34;
        r = kornia.geometry.conversions.axis_angle_to_rotation_matrix(self.rotation.unsqueeze(0))[0].to(device=device,
                                                                                                        dtype=torch.float32)
        rt = torch.hstack([r, self.translation.to(device=device).t().unsqueeze(-1)])
        return torch.vstack([rt, torch.tensor([0., 0., 0., 1.], device=device).unsqueeze(0)])

    def vectorised(self) -&gt; torch.Tensor:
        return torch.cat((self.rotation, self.translation), dim=0)

    @staticmethod
    def from_vector(vector: torch.Tensor) -&gt; &#39;Transformation&#39;:
        assert vector.size() == torch.Size([6])
        return Transformation(rotation=vector[0:3], translation=vector[3:6])

    @property
    def device(self):
        assert self.device_consistent()
        return self.translation.device

    def to(self, **kwargs) -&gt; &#39;Transformation&#39;:
        return Transformation(self.rotation.to(**kwargs), self.translation.to(**kwargs))

    def clone(self) -&gt; &#39;Tranformation&#39;:
        return Transformation(self.rotation.clone(), self.translation.clone())

    def device_consistent(self) -&gt; bool:
        return self.rotation.device == self.translation.device

    def distance(self, other: &#39;Transformation&#39;) -&gt; float:
        device = self.translation.device
        r1 = kornia.geometry.conversions.axis_angle_to_rotation_matrix(self.rotation.unsqueeze(0))[0].to(device=device,
                                                                                                         dtype=torch.float32)
        r2 = kornia.geometry.conversions.axis_angle_to_rotation_matrix(other.rotation.unsqueeze(0))[0].to(device=device,
                                                                                                          dtype=torch.float32)
        return (((self.translation - other.translation) / 100.).square().sum() + torch.tensor(
            numpy.array([numpy.real(scipy.linalg.logm((torch.matmul(r1.t(), r2).cpu().numpy())))]),
            device=device).square().sum()).sqrt().item()

    def is_close(self, other: &#39;Transformation&#39;) -&gt; bool:
        return torch.allclose(self.rotation, other.rotation.to(device=self.rotation.device)) and torch.allclose(
            self.translation, other.translation.to(device=self.translation.device))

    def __str__(self) -&gt; str:
        return &#34;Transformation(rot = {}, trans = {})&#34;.format(str(self.rotation), str(self.translation))

    @classmethod
    def zero(cls, *, device=torch.device(&#39;cpu&#39;)) -&gt; &#39;Transformation&#39;:
        return Transformation(torch.zeros(3, device=device), torch.zeros(3, device=device))

    @classmethod
    def random_uniform(cls, *, device=torch.device(&#39;cpu&#39;)) -&gt; &#39;Transformation&#39;:
        return Transformation(rotation=torch.pi * (-1. + 2. * torch.rand(3, device=device)),
                              translation=25. * (-1. + 2. * torch.rand(3, device=device)) + Transformation.zero(
                                  device=device).translation)

    @classmethod
    def random_gaussian(cls, *, rotation_mean: torch.Tensor, rotation_std: Union[torch.Tensor, float],
                        translation_mean: torch.Tensor, translation_std: Union[torch.Tensor, float],
                        generator=None) -&gt; &#39;Transformation&#39;:
        assert rotation_mean.size() == torch.Size([3])
        assert translation_mean.size() == torch.Size([3])
        assert translation_mean.device == rotation_mean.device
        assert isinstance(rotation_std, float) or (
                (rotation_std.size() == torch.Size([3])) and (rotation_std.device == rotation_mean.device))
        assert isinstance(translation_std, float) or (
                (translation_std.size() == torch.Size([3])) and (translation_std.device == rotation_mean.device))
        return Transformation(rotation=torch.normal(mean=rotation_mean, std=rotation_std, generator=generator),
                              translation=torch.normal(mean=translation_mean, std=translation_std, generator=generator))</code></pre>
</details>
<div class="desc"><p>Transformation(rotation, translation)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="registration.lib.structs.Transformation.from_vector"><code class="name flex">
<span>def <span class="ident">from_vector</span></span>(<span>vector: torch.Tensor) ‑> <a title="registration.lib.structs.Transformation" href="#registration.lib.structs.Transformation">Transformation</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_vector(vector: torch.Tensor) -&gt; &#39;Transformation&#39;:
    assert vector.size() == torch.Size([6])
    return Transformation(rotation=vector[0:3], translation=vector[3:6])</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Transformation.random_gaussian"><code class="name flex">
<span>def <span class="ident">random_gaussian</span></span>(<span>*,<br>rotation_mean: torch.Tensor,<br>rotation_std: float | torch.Tensor,<br>translation_mean: torch.Tensor,<br>translation_std: float | torch.Tensor,<br>generator=None) ‑> <a title="registration.lib.structs.Transformation" href="#registration.lib.structs.Transformation">Transformation</a></span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Transformation.random_uniform"><code class="name flex">
<span>def <span class="ident">random_uniform</span></span>(<span>*, device=device(type='cpu')) ‑> <a title="registration.lib.structs.Transformation" href="#registration.lib.structs.Transformation">Transformation</a></span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Transformation.zero"><code class="name flex">
<span>def <span class="ident">zero</span></span>(<span>*, device=device(type='cpu')) ‑> <a title="registration.lib.structs.Transformation" href="#registration.lib.structs.Transformation">Transformation</a></span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="registration.lib.structs.Transformation.device"><code class="name">prop <span class="ident">device</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def device(self):
    assert self.device_consistent()
    return self.translation.device</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Transformation.rotation"><code class="name">var <span class="ident">rotation</span> : torch.Tensor</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Transformation(NamedTuple):
    rotation: torch.Tensor
    translation: torch.Tensor

    def inverse(self) -&gt; &#39;Transformation&#39;:
        r_inverse = kornia.geometry.conversions.axis_angle_to_rotation_matrix(-self.rotation.unsqueeze(0))[0]
        r_inverse_t = torch.einsum(&#39;kl,...l-&gt;...k&#39;, r_inverse, self.translation.unsqueeze(0))[0]
        return Transformation(-self.rotation, -r_inverse_t)

    def get_h(self, *, device=torch.device(&#39;cpu&#39;)) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        :param device:
        :return: [(4, 4) tensor] The homogenous affine transformation matrix H corresponding to this transformation.
        Stored column-major.
        &#34;&#34;&#34;
        r = kornia.geometry.conversions.axis_angle_to_rotation_matrix(self.rotation.unsqueeze(0))[0].to(device=device,
                                                                                                        dtype=torch.float32)
        rt = torch.hstack([r, self.translation.to(device=device).t().unsqueeze(-1)])
        return torch.vstack([rt, torch.tensor([0., 0., 0., 1.], device=device).unsqueeze(0)])

    def vectorised(self) -&gt; torch.Tensor:
        return torch.cat((self.rotation, self.translation), dim=0)

    @staticmethod
    def from_vector(vector: torch.Tensor) -&gt; &#39;Transformation&#39;:
        assert vector.size() == torch.Size([6])
        return Transformation(rotation=vector[0:3], translation=vector[3:6])

    @property
    def device(self):
        assert self.device_consistent()
        return self.translation.device

    def to(self, **kwargs) -&gt; &#39;Transformation&#39;:
        return Transformation(self.rotation.to(**kwargs), self.translation.to(**kwargs))

    def clone(self) -&gt; &#39;Tranformation&#39;:
        return Transformation(self.rotation.clone(), self.translation.clone())

    def device_consistent(self) -&gt; bool:
        return self.rotation.device == self.translation.device

    def distance(self, other: &#39;Transformation&#39;) -&gt; float:
        device = self.translation.device
        r1 = kornia.geometry.conversions.axis_angle_to_rotation_matrix(self.rotation.unsqueeze(0))[0].to(device=device,
                                                                                                         dtype=torch.float32)
        r2 = kornia.geometry.conversions.axis_angle_to_rotation_matrix(other.rotation.unsqueeze(0))[0].to(device=device,
                                                                                                          dtype=torch.float32)
        return (((self.translation - other.translation) / 100.).square().sum() + torch.tensor(
            numpy.array([numpy.real(scipy.linalg.logm((torch.matmul(r1.t(), r2).cpu().numpy())))]),
            device=device).square().sum()).sqrt().item()

    def is_close(self, other: &#39;Transformation&#39;) -&gt; bool:
        return torch.allclose(self.rotation, other.rotation.to(device=self.rotation.device)) and torch.allclose(
            self.translation, other.translation.to(device=self.translation.device))

    def __str__(self) -&gt; str:
        return &#34;Transformation(rot = {}, trans = {})&#34;.format(str(self.rotation), str(self.translation))

    @classmethod
    def zero(cls, *, device=torch.device(&#39;cpu&#39;)) -&gt; &#39;Transformation&#39;:
        return Transformation(torch.zeros(3, device=device), torch.zeros(3, device=device))

    @classmethod
    def random_uniform(cls, *, device=torch.device(&#39;cpu&#39;)) -&gt; &#39;Transformation&#39;:
        return Transformation(rotation=torch.pi * (-1. + 2. * torch.rand(3, device=device)),
                              translation=25. * (-1. + 2. * torch.rand(3, device=device)) + Transformation.zero(
                                  device=device).translation)

    @classmethod
    def random_gaussian(cls, *, rotation_mean: torch.Tensor, rotation_std: Union[torch.Tensor, float],
                        translation_mean: torch.Tensor, translation_std: Union[torch.Tensor, float],
                        generator=None) -&gt; &#39;Transformation&#39;:
        assert rotation_mean.size() == torch.Size([3])
        assert translation_mean.size() == torch.Size([3])
        assert translation_mean.device == rotation_mean.device
        assert isinstance(rotation_std, float) or (
                (rotation_std.size() == torch.Size([3])) and (rotation_std.device == rotation_mean.device))
        assert isinstance(translation_std, float) or (
                (translation_std.size() == torch.Size([3])) and (translation_std.device == rotation_mean.device))
        return Transformation(rotation=torch.normal(mean=rotation_mean, std=rotation_std, generator=generator),
                              translation=torch.normal(mean=translation_mean, std=translation_std, generator=generator))</code></pre>
</details>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
<dt id="registration.lib.structs.Transformation.translation"><code class="name">var <span class="ident">translation</span> : torch.Tensor</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Transformation(NamedTuple):
    rotation: torch.Tensor
    translation: torch.Tensor

    def inverse(self) -&gt; &#39;Transformation&#39;:
        r_inverse = kornia.geometry.conversions.axis_angle_to_rotation_matrix(-self.rotation.unsqueeze(0))[0]
        r_inverse_t = torch.einsum(&#39;kl,...l-&gt;...k&#39;, r_inverse, self.translation.unsqueeze(0))[0]
        return Transformation(-self.rotation, -r_inverse_t)

    def get_h(self, *, device=torch.device(&#39;cpu&#39;)) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        :param device:
        :return: [(4, 4) tensor] The homogenous affine transformation matrix H corresponding to this transformation.
        Stored column-major.
        &#34;&#34;&#34;
        r = kornia.geometry.conversions.axis_angle_to_rotation_matrix(self.rotation.unsqueeze(0))[0].to(device=device,
                                                                                                        dtype=torch.float32)
        rt = torch.hstack([r, self.translation.to(device=device).t().unsqueeze(-1)])
        return torch.vstack([rt, torch.tensor([0., 0., 0., 1.], device=device).unsqueeze(0)])

    def vectorised(self) -&gt; torch.Tensor:
        return torch.cat((self.rotation, self.translation), dim=0)

    @staticmethod
    def from_vector(vector: torch.Tensor) -&gt; &#39;Transformation&#39;:
        assert vector.size() == torch.Size([6])
        return Transformation(rotation=vector[0:3], translation=vector[3:6])

    @property
    def device(self):
        assert self.device_consistent()
        return self.translation.device

    def to(self, **kwargs) -&gt; &#39;Transformation&#39;:
        return Transformation(self.rotation.to(**kwargs), self.translation.to(**kwargs))

    def clone(self) -&gt; &#39;Tranformation&#39;:
        return Transformation(self.rotation.clone(), self.translation.clone())

    def device_consistent(self) -&gt; bool:
        return self.rotation.device == self.translation.device

    def distance(self, other: &#39;Transformation&#39;) -&gt; float:
        device = self.translation.device
        r1 = kornia.geometry.conversions.axis_angle_to_rotation_matrix(self.rotation.unsqueeze(0))[0].to(device=device,
                                                                                                         dtype=torch.float32)
        r2 = kornia.geometry.conversions.axis_angle_to_rotation_matrix(other.rotation.unsqueeze(0))[0].to(device=device,
                                                                                                          dtype=torch.float32)
        return (((self.translation - other.translation) / 100.).square().sum() + torch.tensor(
            numpy.array([numpy.real(scipy.linalg.logm((torch.matmul(r1.t(), r2).cpu().numpy())))]),
            device=device).square().sum()).sqrt().item()

    def is_close(self, other: &#39;Transformation&#39;) -&gt; bool:
        return torch.allclose(self.rotation, other.rotation.to(device=self.rotation.device)) and torch.allclose(
            self.translation, other.translation.to(device=self.translation.device))

    def __str__(self) -&gt; str:
        return &#34;Transformation(rot = {}, trans = {})&#34;.format(str(self.rotation), str(self.translation))

    @classmethod
    def zero(cls, *, device=torch.device(&#39;cpu&#39;)) -&gt; &#39;Transformation&#39;:
        return Transformation(torch.zeros(3, device=device), torch.zeros(3, device=device))

    @classmethod
    def random_uniform(cls, *, device=torch.device(&#39;cpu&#39;)) -&gt; &#39;Transformation&#39;:
        return Transformation(rotation=torch.pi * (-1. + 2. * torch.rand(3, device=device)),
                              translation=25. * (-1. + 2. * torch.rand(3, device=device)) + Transformation.zero(
                                  device=device).translation)

    @classmethod
    def random_gaussian(cls, *, rotation_mean: torch.Tensor, rotation_std: Union[torch.Tensor, float],
                        translation_mean: torch.Tensor, translation_std: Union[torch.Tensor, float],
                        generator=None) -&gt; &#39;Transformation&#39;:
        assert rotation_mean.size() == torch.Size([3])
        assert translation_mean.size() == torch.Size([3])
        assert translation_mean.device == rotation_mean.device
        assert isinstance(rotation_std, float) or (
                (rotation_std.size() == torch.Size([3])) and (rotation_std.device == rotation_mean.device))
        assert isinstance(translation_std, float) or (
                (translation_std.size() == torch.Size([3])) and (translation_std.device == rotation_mean.device))
        return Transformation(rotation=torch.normal(mean=rotation_mean, std=rotation_std, generator=generator),
                              translation=torch.normal(mean=translation_mean, std=translation_std, generator=generator))</code></pre>
</details>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="registration.lib.structs.Transformation.clone"><code class="name flex">
<span>def <span class="ident">clone</span></span>(<span>self) ‑> Tranformation</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clone(self) -&gt; &#39;Tranformation&#39;:
    return Transformation(self.rotation.clone(), self.translation.clone())</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Transformation.device_consistent"><code class="name flex">
<span>def <span class="ident">device_consistent</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def device_consistent(self) -&gt; bool:
    return self.rotation.device == self.translation.device</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Transformation.distance"><code class="name flex">
<span>def <span class="ident">distance</span></span>(<span>self,<br>other: <a title="registration.lib.structs.Transformation" href="#registration.lib.structs.Transformation">Transformation</a>) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def distance(self, other: &#39;Transformation&#39;) -&gt; float:
    device = self.translation.device
    r1 = kornia.geometry.conversions.axis_angle_to_rotation_matrix(self.rotation.unsqueeze(0))[0].to(device=device,
                                                                                                     dtype=torch.float32)
    r2 = kornia.geometry.conversions.axis_angle_to_rotation_matrix(other.rotation.unsqueeze(0))[0].to(device=device,
                                                                                                      dtype=torch.float32)
    return (((self.translation - other.translation) / 100.).square().sum() + torch.tensor(
        numpy.array([numpy.real(scipy.linalg.logm((torch.matmul(r1.t(), r2).cpu().numpy())))]),
        device=device).square().sum()).sqrt().item()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Transformation.get_h"><code class="name flex">
<span>def <span class="ident">get_h</span></span>(<span>self, *, device=device(type='cpu')) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_h(self, *, device=torch.device(&#39;cpu&#39;)) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    :param device:
    :return: [(4, 4) tensor] The homogenous affine transformation matrix H corresponding to this transformation.
    Stored column-major.
    &#34;&#34;&#34;
    r = kornia.geometry.conversions.axis_angle_to_rotation_matrix(self.rotation.unsqueeze(0))[0].to(device=device,
                                                                                                    dtype=torch.float32)
    rt = torch.hstack([r, self.translation.to(device=device).t().unsqueeze(-1)])
    return torch.vstack([rt, torch.tensor([0., 0., 0., 1.], device=device).unsqueeze(0)])</code></pre>
</details>
<div class="desc"><p>:param device:
:return: [(4, 4) tensor] The homogenous affine transformation matrix H corresponding to this transformation.
Stored column-major.</p></div>
</dd>
<dt id="registration.lib.structs.Transformation.inverse"><code class="name flex">
<span>def <span class="ident">inverse</span></span>(<span>self) ‑> <a title="registration.lib.structs.Transformation" href="#registration.lib.structs.Transformation">Transformation</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse(self) -&gt; &#39;Transformation&#39;:
    r_inverse = kornia.geometry.conversions.axis_angle_to_rotation_matrix(-self.rotation.unsqueeze(0))[0]
    r_inverse_t = torch.einsum(&#39;kl,...l-&gt;...k&#39;, r_inverse, self.translation.unsqueeze(0))[0]
    return Transformation(-self.rotation, -r_inverse_t)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Transformation.is_close"><code class="name flex">
<span>def <span class="ident">is_close</span></span>(<span>self,<br>other: <a title="registration.lib.structs.Transformation" href="#registration.lib.structs.Transformation">Transformation</a>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_close(self, other: &#39;Transformation&#39;) -&gt; bool:
    return torch.allclose(self.rotation, other.rotation.to(device=self.rotation.device)) and torch.allclose(
        self.translation, other.translation.to(device=self.translation.device))</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Transformation.to"><code class="name flex">
<span>def <span class="ident">to</span></span>(<span>self, **kwargs) ‑> <a title="registration.lib.structs.Transformation" href="#registration.lib.structs.Transformation">Transformation</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to(self, **kwargs) -&gt; &#39;Transformation&#39;:
    return Transformation(self.rotation.to(**kwargs), self.translation.to(**kwargs))</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="registration.lib.structs.Transformation.vectorised"><code class="name flex">
<span>def <span class="ident">vectorised</span></span>(<span>self) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vectorised(self) -&gt; torch.Tensor:
    return torch.cat((self.rotation, self.translation), dim=0)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="registration.lib" href="index.html">registration.lib</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="registration.lib.structs.GrowingTensor" href="#registration.lib.structs.GrowingTensor">GrowingTensor</a></code></h4>
<ul class="">
<li><code><a title="registration.lib.structs.GrowingTensor.get" href="#registration.lib.structs.GrowingTensor.get">get</a></code></li>
<li><code><a title="registration.lib.structs.GrowingTensor.push_back" href="#registration.lib.structs.GrowingTensor.push_back">push_back</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="registration.lib.structs.LinearMapping" href="#registration.lib.structs.LinearMapping">LinearMapping</a></code></h4>
</li>
<li>
<h4><code><a title="registration.lib.structs.LinearRange" href="#registration.lib.structs.LinearRange">LinearRange</a></code></h4>
<ul class="">
<li><code><a title="registration.lib.structs.LinearRange.generate_grid" href="#registration.lib.structs.LinearRange.generate_grid">generate_grid</a></code></li>
<li><code><a title="registration.lib.structs.LinearRange.generate_tex_coord_grid" href="#registration.lib.structs.LinearRange.generate_tex_coord_grid">generate_tex_coord_grid</a></code></li>
<li><code><a title="registration.lib.structs.LinearRange.get_centre" href="#registration.lib.structs.LinearRange.get_centre">get_centre</a></code></li>
<li><code><a title="registration.lib.structs.LinearRange.get_mapping_from" href="#registration.lib.structs.LinearRange.get_mapping_from">get_mapping_from</a></code></li>
<li><code><a title="registration.lib.structs.LinearRange.get_spacing" href="#registration.lib.structs.LinearRange.get_spacing">get_spacing</a></code></li>
<li><code><a title="registration.lib.structs.LinearRange.get_tex_coord_spacing" href="#registration.lib.structs.LinearRange.get_tex_coord_spacing">get_tex_coord_spacing</a></code></li>
<li><code><a title="registration.lib.structs.LinearRange.grid_sample_range" href="#registration.lib.structs.LinearRange.grid_sample_range">grid_sample_range</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="registration.lib.structs.SceneGeometry" href="#registration.lib.structs.SceneGeometry">SceneGeometry</a></code></h4>
<ul class="">
<li><code><a title="registration.lib.structs.SceneGeometry.fixed_image_offset" href="#registration.lib.structs.SceneGeometry.fixed_image_offset">fixed_image_offset</a></code></li>
<li><code><a title="registration.lib.structs.SceneGeometry.projection_matrix" href="#registration.lib.structs.SceneGeometry.projection_matrix">projection_matrix</a></code></li>
<li><code><a title="registration.lib.structs.SceneGeometry.source_distance" href="#registration.lib.structs.SceneGeometry.source_distance">source_distance</a></code></li>
<li><code><a title="registration.lib.structs.SceneGeometry.source_position" href="#registration.lib.structs.SceneGeometry.source_position">source_position</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="registration.lib.structs.Sinogram2dGrid" href="#registration.lib.structs.Sinogram2dGrid">Sinogram2dGrid</a></code></h4>
<ul class="two-column">
<li><code><a title="registration.lib.structs.Sinogram2dGrid.device_consistent" href="#registration.lib.structs.Sinogram2dGrid.device_consistent">device_consistent</a></code></li>
<li><code><a title="registration.lib.structs.Sinogram2dGrid.linear_from_range" href="#registration.lib.structs.Sinogram2dGrid.linear_from_range">linear_from_range</a></code></li>
<li><code><a title="registration.lib.structs.Sinogram2dGrid.phi" href="#registration.lib.structs.Sinogram2dGrid.phi">phi</a></code></li>
<li><code><a title="registration.lib.structs.Sinogram2dGrid.r" href="#registration.lib.structs.Sinogram2dGrid.r">r</a></code></li>
<li><code><a title="registration.lib.structs.Sinogram2dGrid.shifted" href="#registration.lib.structs.Sinogram2dGrid.shifted">shifted</a></code></li>
<li><code><a title="registration.lib.structs.Sinogram2dGrid.size_consistent" href="#registration.lib.structs.Sinogram2dGrid.size_consistent">size_consistent</a></code></li>
<li><code><a title="registration.lib.structs.Sinogram2dGrid.to" href="#registration.lib.structs.Sinogram2dGrid.to">to</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="registration.lib.structs.Sinogram2dRange" href="#registration.lib.structs.Sinogram2dRange">Sinogram2dRange</a></code></h4>
<ul class="">
<li><code><a title="registration.lib.structs.Sinogram2dRange.phi" href="#registration.lib.structs.Sinogram2dRange.phi">phi</a></code></li>
<li><code><a title="registration.lib.structs.Sinogram2dRange.r" href="#registration.lib.structs.Sinogram2dRange.r">r</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="registration.lib.structs.Sinogram3dGrid" href="#registration.lib.structs.Sinogram3dGrid">Sinogram3dGrid</a></code></h4>
<ul class="two-column">
<li><code><a title="registration.lib.structs.Sinogram3dGrid.device_consistent" href="#registration.lib.structs.Sinogram3dGrid.device_consistent">device_consistent</a></code></li>
<li><code><a title="registration.lib.structs.Sinogram3dGrid.phi" href="#registration.lib.structs.Sinogram3dGrid.phi">phi</a></code></li>
<li><code><a title="registration.lib.structs.Sinogram3dGrid.r" href="#registration.lib.structs.Sinogram3dGrid.r">r</a></code></li>
<li><code><a title="registration.lib.structs.Sinogram3dGrid.size_consistent" href="#registration.lib.structs.Sinogram3dGrid.size_consistent">size_consistent</a></code></li>
<li><code><a title="registration.lib.structs.Sinogram3dGrid.theta" href="#registration.lib.structs.Sinogram3dGrid.theta">theta</a></code></li>
<li><code><a title="registration.lib.structs.Sinogram3dGrid.to" href="#registration.lib.structs.Sinogram3dGrid.to">to</a></code></li>
<li><code><a title="registration.lib.structs.Sinogram3dGrid.unflip" href="#registration.lib.structs.Sinogram3dGrid.unflip">unflip</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="registration.lib.structs.Transformation" href="#registration.lib.structs.Transformation">Transformation</a></code></h4>
<ul class="two-column">
<li><code><a title="registration.lib.structs.Transformation.clone" href="#registration.lib.structs.Transformation.clone">clone</a></code></li>
<li><code><a title="registration.lib.structs.Transformation.device" href="#registration.lib.structs.Transformation.device">device</a></code></li>
<li><code><a title="registration.lib.structs.Transformation.device_consistent" href="#registration.lib.structs.Transformation.device_consistent">device_consistent</a></code></li>
<li><code><a title="registration.lib.structs.Transformation.distance" href="#registration.lib.structs.Transformation.distance">distance</a></code></li>
<li><code><a title="registration.lib.structs.Transformation.from_vector" href="#registration.lib.structs.Transformation.from_vector">from_vector</a></code></li>
<li><code><a title="registration.lib.structs.Transformation.get_h" href="#registration.lib.structs.Transformation.get_h">get_h</a></code></li>
<li><code><a title="registration.lib.structs.Transformation.inverse" href="#registration.lib.structs.Transformation.inverse">inverse</a></code></li>
<li><code><a title="registration.lib.structs.Transformation.is_close" href="#registration.lib.structs.Transformation.is_close">is_close</a></code></li>
<li><code><a title="registration.lib.structs.Transformation.random_gaussian" href="#registration.lib.structs.Transformation.random_gaussian">random_gaussian</a></code></li>
<li><code><a title="registration.lib.structs.Transformation.random_uniform" href="#registration.lib.structs.Transformation.random_uniform">random_uniform</a></code></li>
<li><code><a title="registration.lib.structs.Transformation.rotation" href="#registration.lib.structs.Transformation.rotation">rotation</a></code></li>
<li><code><a title="registration.lib.structs.Transformation.to" href="#registration.lib.structs.Transformation.to">to</a></code></li>
<li><code><a title="registration.lib.structs.Transformation.translation" href="#registration.lib.structs.Transformation.translation">translation</a></code></li>
<li><code><a title="registration.lib.structs.Transformation.vectorised" href="#registration.lib.structs.Transformation.vectorised">vectorised</a></code></li>
<li><code><a title="registration.lib.structs.Transformation.zero" href="#registration.lib.structs.Transformation.zero">zero</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
